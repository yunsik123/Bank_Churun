{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9220bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Xgboost쓴다.(베이지안최적화)\n",
    "#2.마지막에 공평하게 50퍼씩 가중치주어서 최종 확률 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df7dfe96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165034, 14) (110023, 13) (110023, 2)\n"
     ]
    }
   ],
   "source": [
    "#베이스라인만들어보자\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#train=pd.read_csv(\"/kaggle/input/playground-series-s4e1/train.csv\")\n",
    "#test=pd.read_csv(\"/kaggle/input/playground-series-s4e1/test.csv\")\n",
    "#submission=pd.read_csv(\"/kaggle/input/playground-series-s4e1/sample_submission.csv\")\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")\n",
    "submission=pd.read_csv(\"sample_submission.csv\")\n",
    "#ROC이용\n",
    "print(train.shape,test.shape,submission.shape)\n",
    "\n",
    "#Exited대신 target사용하겠습니다\n",
    "train.rename(columns={'Exited':'target'}, inplace=True)\n",
    "test.rename(columns={'Exited':'target'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00826c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 275057 entries, 0 to 275056\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   id               275057 non-null  int64  \n",
      " 1   CustomerId       275057 non-null  int64  \n",
      " 2   Surname          275057 non-null  object \n",
      " 3   CreditScore      275057 non-null  int64  \n",
      " 4   Geography        275057 non-null  object \n",
      " 5   Gender           275057 non-null  object \n",
      " 6   Age              275057 non-null  float64\n",
      " 7   Tenure           275057 non-null  int64  \n",
      " 8   Balance          275057 non-null  float64\n",
      " 9   NumOfProducts    275057 non-null  int64  \n",
      " 10  HasCrCard        275057 non-null  float64\n",
      " 11  IsActiveMember   275057 non-null  float64\n",
      " 12  EstimatedSalary  275057 non-null  float64\n",
      "dtypes: float64(5), int64(5), object(3)\n",
      "memory usage: 27.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#데이터 합치기\n",
    "all_data=pd.concat([train,test],ignore_index=True)\n",
    "all_data=all_data.drop('target',axis=1)#타깃값제거\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495ec03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<275057x2904 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3072713 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature engineering\n",
    "#간단한 전처리(feature engineering)\n",
    "df=all_data\n",
    "\n",
    "#ordinal encoding은 label도 순서지만 내가 원하는 순서대로 인코딩하고 싶을땐 map함수이용해서하거나 아니면 알파벳은 ordinal encoder쓰면\n",
    "#편함 그냥 라벨은 순서가 고려되긴하는데 막 넣을수있기에 조심해서보고해야함.\n",
    "#sklearn의 모델이나 xgboost는 반드시 레이블이나 원핫 해줘야함 \n",
    "#lightgbm ,catboost ,tree기반 ,statsmodel은 알아서 해줌(더미변수화든 원핫이든 레이블이든)\n",
    "\n",
    "#목적변수는 int형으로 가만히 두기\n",
    "#Xgboost를 R에서 사용할때는 상관 없지만 Python에서 사용할때는 \n",
    "#범주형 변수들을 (Object나 Category로 되어 있는 변수) 숫자형 변수로 바꿔줘야 한다.\n",
    "#그래서 0과1로되어있는 이진변수(int)까지는 숫자형으로 바꿀필요없는듯(xgboost도 마찬가지)\n",
    "#귀찮은거 많이 없애주고 속도도빠르니 lightgbm쓰나벼?\n",
    "\n",
    "#feature engineering 고려해야할 부분!!!!!!!!!!!!!\n",
    "#1.데이터 하나당 결측값 개수를 파생피처로 추가할수 있다.(우리는 아쉽게도 결측값 안존재)\n",
    "#2.명목형 피처의 고유값 별 개수를 만들어볼 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "#중복값이면 1아니면 0인 feature를 df에 추가한다.\n",
    "df['duple'] = df['CustomerId'].duplicated(keep=False).astype('int')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3.Geography와 Gender는 object지만 나중을 위해 숫자로 변환\n",
    "#3개니까 Geography 원핫 인코딩 진행(순서를 고려해야할경우는 label인코딩을 해야함)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "df_encoded = pd.DataFrame(encoder.fit_transform(df[['Geography']]), columns=encoder.get_feature_names_out(['Geography']))\n",
    "# 기존 데이터프레임에서 'Geography' 열 제거\n",
    "df = df.drop(['Geography'], axis=1)\n",
    "# 인코딩된 결과를 기존 데이터프레임에 합치기\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "\n",
    "#2개니까 gender는 원핫인코딩필요 없음 레이블 인코디진행 0과1이잖아\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "df['Gender'] = df['Gender'].astype('int')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#5.Surname과 관련된 onehot인코딩\n",
    "#6.CustomerId 와 관련된 onehot인코딩\n",
    "#위의 두개를 한번에 적용한다.한번에 적용해지면 너무 커서 메모리할당안됨....어쩌지?\n",
    "#cat_features=['Surname','CustomerId']\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "#encoded_cat_matrix=encoder.fit_transform(df[cat_features])\n",
    "#encoded_cat_matrix\n",
    "\n",
    "\n",
    "#7.id없애기\n",
    "df = df.drop(['id'], axis=1)\n",
    "\n",
    "#8.Balance 0인지 아닌지에 대한 feature생성\n",
    "df['Balance2'] = (df['Balance'] == 0).astype('int')\n",
    "\n",
    "#9 CreditScore 850인지 아닌지에 대한 feature 생성\n",
    "df['CreditScore2'] = (df['CreditScore'] == 0).astype('int')\n",
    "\n",
    "#10 Balance가 20000보다 크냐 아니냐에 따라 feature생성\n",
    "df['Balance3'] = (df['Balance'] > 200000).astype(int)\n",
    "\n",
    "\n",
    "#6.따로따로 한다음에 합칠때 csr형식으로 합쳐보자\n",
    "#따로따로해도 크기 때문에 둘중 선택해야함(해봄)\n",
    "#CustomerId는 혼자해도 커서 일단 포기\n",
    "\n",
    "\n",
    "\n",
    "#CustomerId 인코딩(원핫안되니까 라벨씀..타깃을 이럴때 써야하나싶음 쩔수임)\n",
    "label_encoder = LabelEncoder()\n",
    "df['CustomerId'] = label_encoder.fit_transform(df['CustomerId'])\n",
    "\n",
    "\n",
    "#Surname 원핫인코딩한다음에 matrix로 저장\n",
    "encoder = OneHotEncoder(sparse_output=True, drop='first')#false는 밀집행렬 true는 csr희소행렬이다. 밑에 보면 sparse.csr_matrix를 안한것을 볼 수 있다.\n",
    "encoded_cat_matrix_1=encoder.fit_transform(df[['Surname']])\n",
    "#Surname원핫인코딩해줬으니 Surname변수 df에서 없애버려야됨 df도 csr해줄려면 (object형은 없어야함 최소 숫자나 범주)\n",
    "df = df.drop(['Surname'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#원래데이터와 Surname 데이터를 수평으로 저장하는데 format이 'csr'임\n",
    "from scipy import sparse\n",
    "all_data_sprs=sparse.hstack([sparse.csr_matrix(df),\n",
    "                             encoded_cat_matrix_1],\n",
    "                             format='csr')\n",
    "\n",
    "all_data_sprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d962a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values\n",
    "\n",
    "#평가지표 계산함수 작성\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# ROC AUC 계산\n",
    "#roc_auc = roc_auc_score(y_true, y_scores)\n",
    "#y_true는 실제 값 y_scores는 예측 확률값이다.\n",
    "\n",
    "\n",
    "#Lightgbm용 roc_auc 계산 함수\n",
    "def roc_auc(preds,dtrain):\n",
    "    labels=dtrain.get_label()\n",
    "    return 'roc_auc', roc_auc_score(labels,preds), True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abb9e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#베이지안 최적화 준비\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves': (30, 40),\n",
    "                'lambda_l1': (0.7, 0.9),\n",
    "                'lambda_l2': (0.9, 1),\n",
    "                'feature_fraction': (0.6, 0.7),\n",
    "                'bagging_fraction': (0.6, 0.9),\n",
    "                'min_child_samples': (6, 10),\n",
    "                'min_child_weight': (10, 40)}\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "fixed_params = {'objective': 'binary',\n",
    "                'learning_rate': 0.005,\n",
    "                'bagging_freq': 1,\n",
    "                'force_row_wise': True,\n",
    "                'random_state': 1991}\n",
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화하려는 평가지표 계산 함수'''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터 \n",
    "    params = {'num_leaves': int(round(num_leaves)),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'min_child_samples': int(round(min_child_samples)),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'feature_pre_filter': False}\n",
    "    # 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    print('하이퍼파라미터:', params)    \n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=params, \n",
    "                           train_set=bayes_dtrain,\n",
    "                           num_boost_round=2500,\n",
    "                           valid_sets=bayes_dvalid,\n",
    "                           feval=roc_auc,\n",
    "                           early_stopping_rounds=300,\n",
    "                           verbose_eval=False)\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid) \n",
    "    # rocauc 계산\n",
    "    roc_score = roc_auc_score(y_valid, preds)\n",
    "    print(f'roc 점수 : {roc_score}\\n')\n",
    "    \n",
    "    return roc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24b8543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n",
      "roc 점수 : 0.8912956409912415\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8913   \u001b[0m | \u001b[0m0.7646   \u001b[0m | \u001b[0m0.6715   \u001b[0m | \u001b[0m0.8206   \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m7.695    \u001b[0m | \u001b[0m29.38    \u001b[0m | \u001b[0m34.38    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc 점수 : 0.891971139938427\n",
      "\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.892    \u001b[0m | \u001b[95m0.8675   \u001b[0m | \u001b[95m0.6964   \u001b[0m | \u001b[95m0.7767   \u001b[0m | \u001b[95m0.9792   \u001b[0m | \u001b[95m8.116    \u001b[0m | \u001b[95m27.04    \u001b[0m | \u001b[95m39.26    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "roc 점수 : 0.8902280818002718\n",
      "\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8902   \u001b[0m | \u001b[0m0.6213   \u001b[0m | \u001b[0m0.6087   \u001b[0m | \u001b[0m0.704    \u001b[0m | \u001b[0m0.9833   \u001b[0m | \u001b[0m9.113    \u001b[0m | \u001b[0m36.1     \u001b[0m | \u001b[0m39.79    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 38, 'lambda_l1': 0.8272987855228503, 'lambda_l2': 0.9698906413240835, 'feature_fraction': 0.6140630948592485, 'bagging_fraction': 0.8832715987739463, 'min_child_samples': 8, 'min_child_weight': 24.840466933833497, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "roc 점수 : 0.8924660822017281\n",
      "\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.8925   \u001b[0m | \u001b[95m0.8833   \u001b[0m | \u001b[95m0.6141   \u001b[0m | \u001b[95m0.8273   \u001b[0m | \u001b[95m0.9699   \u001b[0m | \u001b[95m7.778    \u001b[0m | \u001b[95m24.84    \u001b[0m | \u001b[95m37.56    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.87376591904404, 'lambda_l2': 0.9231468596010995, 'feature_fraction': 0.6117181008140015, 'bagging_fraction': 0.798840537814631, 'min_child_samples': 7, 'min_child_weight': 17.965330410768573, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "roc 점수 : 0.8928099770069031\n",
      "\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8928   \u001b[0m | \u001b[95m0.7988   \u001b[0m | \u001b[95m0.6117   \u001b[0m | \u001b[95m0.8738   \u001b[0m | \u001b[95m0.9231   \u001b[0m | \u001b[95m7.18     \u001b[0m | \u001b[95m17.97    \u001b[0m | \u001b[95m39.95    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.7, 'lambda_l2': 1.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'min_child_samples': 10, 'min_child_weight': 12.447036722663336, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc 점수 : 0.8930085195541929\n",
      "\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.893    \u001b[0m | \u001b[95m0.9      \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m12.45    \u001b[0m | \u001b[95m30.0     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 38, 'lambda_l1': 0.7, 'lambda_l2': 1.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 10.0, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc 점수 : 0.8932035829324644\n",
      "\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.8932   \u001b[0m | \u001b[95m0.6      \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m38.19    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.7, 'lambda_l2': 0.9, 'feature_fraction': 0.7, 'bagging_fraction': 0.6, 'min_child_samples': 6, 'min_child_weight': 10.0, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc 점수 : 0.8932049347115099\n",
      "\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.8932   \u001b[0m | \u001b[95m0.6      \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m0.9      \u001b[0m | \u001b[95m6.0      \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m33.41    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.7032881500564242, 'lambda_l2': 0.9950736372987603, 'feature_fraction': 0.6919553690043856, 'bagging_fraction': 0.6204662597322463, 'min_child_samples': 6, 'min_child_weight': 10.048543421515697, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 28002, number of negative: 104025\n",
      "[LightGBM] [Info] Total Bins 6612\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 2761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.212093 -> initscore=-1.312355\n",
      "[LightGBM] [Info] Start training from score -1.312355\n",
      "roc 점수 : 0.8932023585752144\n",
      "\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.8932   \u001b[0m | \u001b[0m0.6205   \u001b[0m | \u001b[0m0.692    \u001b[0m | \u001b[0m0.7033   \u001b[0m | \u001b[0m0.9951   \u001b[0m | \u001b[0m6.014    \u001b[0m | \u001b[0m10.05    \u001b[0m | \u001b[0m33.55    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#최적화 수행\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n",
    "                                 random_state=0)\n",
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943db396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6,\n",
       " 'feature_fraction': 0.7,\n",
       " 'lambda_l1': 0.7,\n",
       " 'lambda_l2': 0.9,\n",
       " 'min_child_samples': 6.0,\n",
       " 'min_child_weight': 10.0,\n",
       " 'num_leaves': 33.41386040721548}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#결과확인\n",
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5827390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6,\n",
       " 'feature_fraction': 0.7,\n",
       " 'lambda_l1': 0.7,\n",
       " 'lambda_l2': 0.9,\n",
       " 'min_child_samples': 6,\n",
       " 'min_child_weight': 10.0,\n",
       " 'num_leaves': 33,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))\n",
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b24cea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n",
      "[LightGBM] [Info] Total Bins 4513\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 1712\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n",
      "[LightGBM] [Info] Start training from score -1.315304\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.421762\tvalid_0's roc_auc: 0.888626\n",
      "[200]\tvalid_0's binary_logloss: 0.375763\tvalid_0's roc_auc: 0.889236\n",
      "[300]\tvalid_0's binary_logloss: 0.351166\tvalid_0's roc_auc: 0.8895\n",
      "[400]\tvalid_0's binary_logloss: 0.337763\tvalid_0's roc_auc: 0.889812\n",
      "[500]\tvalid_0's binary_logloss: 0.33036\tvalid_0's roc_auc: 0.890117\n",
      "[600]\tvalid_0's binary_logloss: 0.325693\tvalid_0's roc_auc: 0.890417\n",
      "[700]\tvalid_0's binary_logloss: 0.32269\tvalid_0's roc_auc: 0.890716\n",
      "[800]\tvalid_0's binary_logloss: 0.32078\tvalid_0's roc_auc: 0.891126\n",
      "[900]\tvalid_0's binary_logloss: 0.319294\tvalid_0's roc_auc: 0.891632\n",
      "[1000]\tvalid_0's binary_logloss: 0.318151\tvalid_0's roc_auc: 0.8921\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.318151\tvalid_0's roc_auc: 0.8921\n",
      "폴드 1 auc넓이 : 0.8920997870242463\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n",
      "[LightGBM] [Info] Total Bins 4522\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 1716\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n",
      "[LightGBM] [Info] Start training from score -1.315304\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.418836\tvalid_0's roc_auc: 0.887516\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\tvalid_0's binary_logloss: 0.373698\tvalid_0's roc_auc: 0.887796\n",
      "[300]\tvalid_0's binary_logloss: 0.350035\tvalid_0's roc_auc: 0.888326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\tvalid_0's binary_logloss: 0.337648\tvalid_0's roc_auc: 0.888731\n",
      "[500]\tvalid_0's binary_logloss: 0.330145\tvalid_0's roc_auc: 0.889096\n",
      "[600]\tvalid_0's binary_logloss: 0.325752\tvalid_0's roc_auc: 0.889402\n",
      "[700]\tvalid_0's binary_logloss: 0.322997\tvalid_0's roc_auc: 0.88971\n",
      "[800]\tvalid_0's binary_logloss: 0.321139\tvalid_0's roc_auc: 0.890112\n",
      "[900]\tvalid_0's binary_logloss: 0.319789\tvalid_0's roc_auc: 0.890585\n",
      "[1000]\tvalid_0's binary_logloss: 0.318692\tvalid_0's roc_auc: 0.891028\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.318692\tvalid_0's roc_auc: 0.891028\n",
      "폴드 2 auc넓이 : 0.8910283303443043\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 27937, number of negative: 104090\n",
      "[LightGBM] [Info] Total Bins 4536\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 1723\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211601 -> initscore=-1.315304\n",
      "[LightGBM] [Info] Start training from score -1.315304\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.416092\tvalid_0's roc_auc: 0.88669\n",
      "[200]\tvalid_0's binary_logloss: 0.3741\tvalid_0's roc_auc: 0.887362\n",
      "[300]\tvalid_0's binary_logloss: 0.350861\tvalid_0's roc_auc: 0.887794\n",
      "[400]\tvalid_0's binary_logloss: 0.338169\tvalid_0's roc_auc: 0.888056\n",
      "[500]\tvalid_0's binary_logloss: 0.33102\tvalid_0's roc_auc: 0.888391\n",
      "[600]\tvalid_0's binary_logloss: 0.326648\tvalid_0's roc_auc: 0.888745\n",
      "[700]\tvalid_0's binary_logloss: 0.323813\tvalid_0's roc_auc: 0.889142\n",
      "[800]\tvalid_0's binary_logloss: 0.321878\tvalid_0's roc_auc: 0.88955\n",
      "[900]\tvalid_0's binary_logloss: 0.320378\tvalid_0's roc_auc: 0.890051\n",
      "[1000]\tvalid_0's binary_logloss: 0.319236\tvalid_0's roc_auc: 0.890561\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.319236\tvalid_0's roc_auc: 0.890561\n",
      "폴드 3 auc넓이 : 0.89056050634827\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 27936, number of negative: 104091\n",
      "[LightGBM] [Info] Total Bins 4547\n",
      "[LightGBM] [Info] Number of data points in the train set: 132027, number of used features: 1728\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211593 -> initscore=-1.315349\n",
      "[LightGBM] [Info] Start training from score -1.315349\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.422308\tvalid_0's roc_auc: 0.88543\n",
      "[200]\tvalid_0's binary_logloss: 0.378334\tvalid_0's roc_auc: 0.886023\n",
      "[300]\tvalid_0's binary_logloss: 0.354546\tvalid_0's roc_auc: 0.886701\n",
      "[400]\tvalid_0's binary_logloss: 0.340532\tvalid_0's roc_auc: 0.887113\n",
      "[500]\tvalid_0's binary_logloss: 0.332411\tvalid_0's roc_auc: 0.887433\n",
      "[600]\tvalid_0's binary_logloss: 0.327668\tvalid_0's roc_auc: 0.887726\n",
      "[700]\tvalid_0's binary_logloss: 0.32489\tvalid_0's roc_auc: 0.888014\n",
      "[800]\tvalid_0's binary_logloss: 0.323061\tvalid_0's roc_auc: 0.888437\n",
      "[900]\tvalid_0's binary_logloss: 0.321659\tvalid_0's roc_auc: 0.888901\n",
      "[1000]\tvalid_0's binary_logloss: 0.320567\tvalid_0's roc_auc: 0.889384\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.320567\tvalid_0's roc_auc: 0.889384\n",
      "폴드 4 auc넓이 : 0.8893836705651905\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 27937, number of negative: 104091\n",
      "[LightGBM] [Info] Total Bins 4484\n",
      "[LightGBM] [Info] Number of data points in the train set: 132028, number of used features: 1697\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.419928\tvalid_0's roc_auc: 0.886046\n",
      "[200]\tvalid_0's binary_logloss: 0.376136\tvalid_0's roc_auc: 0.886524\n",
      "[300]\tvalid_0's binary_logloss: 0.352452\tvalid_0's roc_auc: 0.886821\n",
      "[400]\tvalid_0's binary_logloss: 0.339445\tvalid_0's roc_auc: 0.887192\n",
      "[500]\tvalid_0's binary_logloss: 0.331429\tvalid_0's roc_auc: 0.887525\n",
      "[600]\tvalid_0's binary_logloss: 0.326943\tvalid_0's roc_auc: 0.887765\n",
      "[700]\tvalid_0's binary_logloss: 0.324118\tvalid_0's roc_auc: 0.888061\n",
      "[800]\tvalid_0's binary_logloss: 0.322224\tvalid_0's roc_auc: 0.888423\n",
      "[900]\tvalid_0's binary_logloss: 0.320827\tvalid_0's roc_auc: 0.888841\n",
      "[1000]\tvalid_0's binary_logloss: 0.319756\tvalid_0's roc_auc: 0.889303\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.319756\tvalid_0's roc_auc: 0.889303\n",
      "폴드 5 auc넓이 : 0.8893033984901138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#모델훈련및 성능검증\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0]) \n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds_lgb = np.zeros(X_test.shape[0]) \n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정 \n",
    "    X_train, y_train = X[train_idx], y[train_idx]  # 훈련용 데이터 (iloc 사용)\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]  # 검증용 데이터 (iloc 사용)\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성 \n",
    "    dtrain = lgb.Dataset(X_train, y_train)  # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)  # LightGBM 전용 검증 데이터셋\n",
    "    \n",
    "    # LightGBM 모델 훈련 \n",
    "    lgb_model = lgb.train(params=max_params,        # 훈련용 하이퍼파라미터\n",
    "                          train_set=dtrain,     # 훈련 데이터셋\n",
    "                          num_boost_round=1000, # 부스팅 반복 횟수\n",
    "                          valid_sets=dvalid,    # 성능 평가용 검증 데이터셋\n",
    "                          feval=roc_auc,        # 검증용 평가지표\n",
    "                          early_stopping_rounds=100, # 조기종료 조건\n",
    "                          verbose_eval=100)     # 100번째마다 점수 출력\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측(이게 진짜 결과임 5로나눠서 더해주잖아)\n",
    "    oof_test_preds_lgb += lgb_model.predict(X_test)/folds.n_splits\n",
    "    \n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측(각 폴드마다)->5로 안나누어주는 이유는 평균값을 낼이유가 없잖아 폴드마다 결과를 보여\n",
    "    #주기만 하면되는데\n",
    "    oof_val_preds[valid_idx] = lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 검증 데이터 예측 확률에 대한 auc넓이\n",
    "    score = roc_auc_score(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1} auc넓이 : {score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9354ea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test데이터에대한 roc넓이 0.8904299404267852\n"
     ]
    }
   ],
   "source": [
    "print('test데이터에대한 roc넓이', roc_auc_score(y, oof_val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ca7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Exited']=oof_test_preds_lgb\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e6f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af044d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16563a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ffdf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37a2eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0074d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost용 roc_auc 계산 함수  #True가 없어졌는데 '평가점수가 높은지 좋은지 여부'는 train()메서드에서 따로 전달해야한다\n",
    "def roc_auc(preds,dtrain):\n",
    "    labels=dtrain.get_label()\n",
    "    return 'roc_auc', roc_auc_score(labels,preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9e4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d683c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#베이지안 최적화 준비\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "bayes_dtrain = xgb.DMatrix(X_train, y_train) \n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "\n",
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth': (4, 8),\n",
    "                'subsample': (0.6, 0.9),\n",
    "                'colsample_bytree': (0.7, 1.0),\n",
    "                'min_child_weight': (5, 7),\n",
    "                'gamma': (8, 11),\n",
    "                'reg_alpha': (7, 9),\n",
    "                'reg_lambda': (1.1, 1.5),\n",
    "                'scale_pos_weight': (1.4, 1.6)}\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "fixed_params = {'objective': 'binary:logistic',\n",
    "                'learning_rate': 0.02,\n",
    "                'random_state': 1991}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71eed013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(max_depth, subsample, colsample_bytree, min_child_weight,\n",
    "                 reg_alpha, gamma, reg_lambda, scale_pos_weight):\n",
    "    '''최적화하려는 평가지표 계산 함수'''\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "    params = {'max_depth': int(round(max_depth)),\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'gamma': gamma,\n",
    "              'reg_alpha':reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight': scale_pos_weight}\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "    print('하이퍼파라미터 :', params)    \n",
    "        \n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params=params, \n",
    "                          dtrain=bayes_dtrain,\n",
    "                          num_boost_round=2000,\n",
    "                          evals=[(bayes_dvalid, 'bayes_dvalid')],\n",
    "                          maximize=True,\n",
    "                          feval=roc_auc,\n",
    "                          early_stopping_rounds=200,\n",
    "                          verbose_eval=False)\n",
    "                           \n",
    "    best_iter = xgb_model.best_iteration # 최적 반복 횟수를 지정해야줘야함 lightgbm과는 다르게 우리가 밑에다가 넣어줘야지\n",
    "                                            #최적 반복횟수로 훈련된 모델을 활용해 예측하기 때문이다.\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = xgb_model.predict(bayes_dvalid, \n",
    "                              iteration_range=(0, best_iter))\n",
    "    # 지니계수 계산\n",
    "    score = roc_auc_score(y_valid, preds)\n",
    "    print(f'auc넓이 : {score}\\n')\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93e250c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 : {'max_depth': 6, 'subsample': 0.867531900234624, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 6.0897663659937935, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8910729326234765\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.8911   \u001b[0m | \u001b[0m0.8646   \u001b[0m | \u001b[0m10.15    \u001b[0m | \u001b[0m6.411    \u001b[0m | \u001b[0m6.09     \u001b[0m | \u001b[0m7.847    \u001b[0m | \u001b[0m1.358    \u001b[0m | \u001b[0m1.488    \u001b[0m | \u001b[0m0.8675   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6261387899104622, 'colsample_bytree': 0.9890988281503088, 'min_child_weight': 6.0577898395058085, 'gamma': 9.150324556477333, 'reg_alpha': 8.136089122187865, 'reg_lambda': 1.4702386553170643, 'scale_pos_weight': 1.4142072116395774, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8911272558588431\n",
      "\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8911   \u001b[0m | \u001b[95m0.9891   \u001b[0m | \u001b[95m9.15     \u001b[0m | \u001b[95m7.167    \u001b[0m | \u001b[95m6.058    \u001b[0m | \u001b[95m8.136    \u001b[0m | \u001b[95m1.47     \u001b[0m | \u001b[95m1.414    \u001b[0m | \u001b[95m0.6261   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8341587528859367, 'colsample_bytree': 0.7060655192320977, 'min_child_weight': 6.7400242964936385, 'gamma': 10.497859536643814, 'reg_alpha': 8.957236684465528, 'reg_lambda': 1.4196634256866894, 'scale_pos_weight': 1.4922958724505864, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8910862537411591\n",
      "\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8911   \u001b[0m | \u001b[0m0.7061   \u001b[0m | \u001b[0m10.5     \u001b[0m | \u001b[0m7.113    \u001b[0m | \u001b[0m6.74     \u001b[0m | \u001b[0m8.957    \u001b[0m | \u001b[0m1.42     \u001b[0m | \u001b[0m1.492    \u001b[0m | \u001b[0m0.8342   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.7001630536555632, 'colsample_bytree': 0.8843124587484356, 'min_child_weight': 6.494091293383359, 'gamma': 10.452246227672624, 'reg_alpha': 8.551838810159788, 'reg_lambda': 1.3814765995549108, 'scale_pos_weight': 1.423280772455086, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8909571006715062\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.891    \u001b[0m | \u001b[0m0.8843   \u001b[0m | \u001b[0m10.45    \u001b[0m | \u001b[0m6.838    \u001b[0m | \u001b[0m6.494    \u001b[0m | \u001b[0m8.552    \u001b[0m | \u001b[0m1.381    \u001b[0m | \u001b[0m1.423    \u001b[0m | \u001b[0m0.7002   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8535233675350644, 'colsample_bytree': 0.92975858050776, 'min_child_weight': 6.249564429359247, 'gamma': 9.95563546750357, 'reg_alpha': 8.411512219837842, 'reg_lambda': 1.424460008293778, 'scale_pos_weight': 1.5416807226581535, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8910801097945965\n",
      "\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.8911   \u001b[0m | \u001b[0m0.9298   \u001b[0m | \u001b[0m9.956    \u001b[0m | \u001b[0m6.809    \u001b[0m | \u001b[0m6.25     \u001b[0m | \u001b[0m8.412    \u001b[0m | \u001b[0m1.424    \u001b[0m | \u001b[0m1.542    \u001b[0m | \u001b[0m0.8535   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6462619019069298, 'colsample_bytree': 0.80929192865947, 'min_child_weight': 6.079999276892042, 'gamma': 9.553916776586505, 'reg_alpha': 8.860396362258099, 'reg_lambda': 1.4050740023119348, 'scale_pos_weight': 1.4668544695338273, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8910844809161818\n",
      "\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.8911   \u001b[0m | \u001b[0m0.8093   \u001b[0m | \u001b[0m9.554    \u001b[0m | \u001b[0m6.532    \u001b[0m | \u001b[0m6.08     \u001b[0m | \u001b[0m8.86     \u001b[0m | \u001b[0m1.405    \u001b[0m | \u001b[0m1.467    \u001b[0m | \u001b[0m0.6463   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6931141936797243, 'colsample_bytree': 0.8817801730078565, 'min_child_weight': 6.992334203641873, 'gamma': 9.013424730095146, 'reg_alpha': 7.640858389939128, 'reg_lambda': 1.3562805915715632, 'scale_pos_weight': 1.449446257931491, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8914597547234595\n",
      "\n",
      "| \u001b[95m7        \u001b[0m | \u001b[95m0.8915   \u001b[0m | \u001b[95m0.8818   \u001b[0m | \u001b[95m9.013    \u001b[0m | \u001b[95m6.927    \u001b[0m | \u001b[95m6.992    \u001b[0m | \u001b[95m7.641    \u001b[0m | \u001b[95m1.356    \u001b[0m | \u001b[95m1.449    \u001b[0m | \u001b[95m0.6931   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.7341280260309927, 'colsample_bytree': 0.7981716808585182, 'min_child_weight': 7.0, 'gamma': 8.64904417306966, 'reg_alpha': 7.243318991223097, 'reg_lambda': 1.3275790202763766, 'scale_pos_weight': 1.462412224713299, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8916971055175233\n",
      "\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.8917   \u001b[0m | \u001b[95m0.7982   \u001b[0m | \u001b[95m8.649    \u001b[0m | \u001b[95m6.842    \u001b[0m | \u001b[95m7.0      \u001b[0m | \u001b[95m7.243    \u001b[0m | \u001b[95m1.328    \u001b[0m | \u001b[95m1.462    \u001b[0m | \u001b[95m0.7341   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 6, 'subsample': 0.9, 'colsample_bytree': 0.7, 'min_child_weight': 7.0, 'gamma': 8.0, 'reg_alpha': 7.0, 'reg_lambda': 1.1705223230377246, 'scale_pos_weight': 1.6, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc넓이 : 0.8919614087913238\n",
      "\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.892    \u001b[0m | \u001b[95m0.7      \u001b[0m | \u001b[95m8.0      \u001b[0m | \u001b[95m6.25     \u001b[0m | \u001b[95m7.0      \u001b[0m | \u001b[95m7.0      \u001b[0m | \u001b[95m1.171    \u001b[0m | \u001b[95m1.6      \u001b[0m | \u001b[95m0.9      \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#최적화 수행\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f=eval_function, \n",
    "                                 pbounds=param_bounds, \n",
    "                                 random_state=0)\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f28692fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7,\n",
       " 'gamma': 8.0,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 7.0,\n",
       " 'reg_alpha': 7.0,\n",
       " 'reg_lambda': 1.1705223230377246,\n",
       " 'scale_pos_weight': 1.6,\n",
       " 'subsample': 0.9,\n",
       " 'objective': 'binary:logistic',\n",
       " 'learning_rate': 0.02,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#결과\n",
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params\n",
    "\n",
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "max_params.update(fixed_params)\n",
    "max_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa4ce294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.53588\tvalid-roc_auc:0.81090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid-logloss:0.35228\tvalid-roc_auc:0.88986\n",
      "[200]\tvalid-logloss:0.33313\tvalid-roc_auc:0.89084\n",
      "[300]\tvalid-logloss:0.32981\tvalid-roc_auc:0.89196\n",
      "[400]\tvalid-logloss:0.32848\tvalid-roc_auc:0.89267\n",
      "[500]\tvalid-logloss:0.32766\tvalid-roc_auc:0.89315\n",
      "[600]\tvalid-logloss:0.32731\tvalid-roc_auc:0.89337\n",
      "[700]\tvalid-logloss:0.32716\tvalid-roc_auc:0.89346\n",
      "[800]\tvalid-logloss:0.32708\tvalid-roc_auc:0.89353\n",
      "[900]\tvalid-logloss:0.32704\tvalid-roc_auc:0.89356\n",
      "[1000]\tvalid-logloss:0.32698\tvalid-roc_auc:0.89359\n",
      "[1100]\tvalid-logloss:0.32695\tvalid-roc_auc:0.89361\n",
      "[1200]\tvalid-logloss:0.32692\tvalid-roc_auc:0.89362\n",
      "[1300]\tvalid-logloss:0.32689\tvalid-roc_auc:0.89363\n",
      "[1400]\tvalid-logloss:0.32688\tvalid-roc_auc:0.89364\n",
      "[1500]\tvalid-logloss:0.32687\tvalid-roc_auc:0.89364\n",
      "[1600]\tvalid-logloss:0.32686\tvalid-roc_auc:0.89365\n",
      "[1700]\tvalid-logloss:0.32682\tvalid-roc_auc:0.89366\n",
      "[1800]\tvalid-logloss:0.32683\tvalid-roc_auc:0.89367\n",
      "[1900]\tvalid-logloss:0.32682\tvalid-roc_auc:0.89367\n",
      "[1999]\tvalid-logloss:0.32682\tvalid-roc_auc:0.89368\n",
      "폴드 1 점수 : 0.8936798226865925\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.53591\tvalid-roc_auc:0.80806\n",
      "[100]\tvalid-logloss:0.35257\tvalid-roc_auc:0.88853\n",
      "[200]\tvalid-logloss:0.33348\tvalid-roc_auc:0.88957\n",
      "[300]\tvalid-logloss:0.33018\tvalid-roc_auc:0.89060\n",
      "[400]\tvalid-logloss:0.32875\tvalid-roc_auc:0.89141\n",
      "[500]\tvalid-logloss:0.32780\tvalid-roc_auc:0.89206\n",
      "[600]\tvalid-logloss:0.32738\tvalid-roc_auc:0.89232\n",
      "[700]\tvalid-logloss:0.32723\tvalid-roc_auc:0.89243\n",
      "[800]\tvalid-logloss:0.32713\tvalid-roc_auc:0.89251\n",
      "[900]\tvalid-logloss:0.32707\tvalid-roc_auc:0.89255\n",
      "[1000]\tvalid-logloss:0.32702\tvalid-roc_auc:0.89257\n",
      "[1100]\tvalid-logloss:0.32700\tvalid-roc_auc:0.89258\n",
      "[1200]\tvalid-logloss:0.32697\tvalid-roc_auc:0.89261\n",
      "[1300]\tvalid-logloss:0.32694\tvalid-roc_auc:0.89264\n",
      "[1400]\tvalid-logloss:0.32692\tvalid-roc_auc:0.89265\n",
      "[1500]\tvalid-logloss:0.32691\tvalid-roc_auc:0.89266\n",
      "[1600]\tvalid-logloss:0.32693\tvalid-roc_auc:0.89267\n",
      "[1700]\tvalid-logloss:0.32689\tvalid-roc_auc:0.89268\n",
      "[1800]\tvalid-logloss:0.32687\tvalid-roc_auc:0.89270\n",
      "[1900]\tvalid-logloss:0.32684\tvalid-roc_auc:0.89272\n",
      "[1999]\tvalid-logloss:0.32682\tvalid-roc_auc:0.89273\n",
      "폴드 2 점수 : 0.8927256431980891\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.53587\tvalid-roc_auc:0.80999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid-logloss:0.35312\tvalid-roc_auc:0.88794\n",
      "[200]\tvalid-logloss:0.33425\tvalid-roc_auc:0.88913\n",
      "[300]\tvalid-logloss:0.33086\tvalid-roc_auc:0.89030\n",
      "[400]\tvalid-logloss:0.32936\tvalid-roc_auc:0.89120\n",
      "[500]\tvalid-logloss:0.32843\tvalid-roc_auc:0.89176\n",
      "[600]\tvalid-logloss:0.32810\tvalid-roc_auc:0.89198\n",
      "[700]\tvalid-logloss:0.32796\tvalid-roc_auc:0.89208\n",
      "[800]\tvalid-logloss:0.32785\tvalid-roc_auc:0.89216\n",
      "[900]\tvalid-logloss:0.32778\tvalid-roc_auc:0.89220\n",
      "[1000]\tvalid-logloss:0.32774\tvalid-roc_auc:0.89222\n",
      "[1100]\tvalid-logloss:0.32771\tvalid-roc_auc:0.89225\n",
      "[1200]\tvalid-logloss:0.32768\tvalid-roc_auc:0.89228\n",
      "[1300]\tvalid-logloss:0.32765\tvalid-roc_auc:0.89228\n",
      "[1400]\tvalid-logloss:0.32761\tvalid-roc_auc:0.89231\n",
      "[1500]\tvalid-logloss:0.32759\tvalid-roc_auc:0.89232\n",
      "[1600]\tvalid-logloss:0.32759\tvalid-roc_auc:0.89233\n",
      "[1700]\tvalid-logloss:0.32754\tvalid-roc_auc:0.89235\n",
      "[1800]\tvalid-logloss:0.32753\tvalid-roc_auc:0.89236\n",
      "[1900]\tvalid-logloss:0.32753\tvalid-roc_auc:0.89236\n",
      "[1999]\tvalid-logloss:0.32750\tvalid-roc_auc:0.89237\n",
      "폴드 3 점수 : 0.8923707909018188\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.53591\tvalid-roc_auc:0.81134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid-logloss:0.35311\tvalid-roc_auc:0.88715\n",
      "[200]\tvalid-logloss:0.33447\tvalid-roc_auc:0.88817\n",
      "[300]\tvalid-logloss:0.33127\tvalid-roc_auc:0.88924\n",
      "[400]\tvalid-logloss:0.32996\tvalid-roc_auc:0.88999\n",
      "[500]\tvalid-logloss:0.32914\tvalid-roc_auc:0.89053\n",
      "[600]\tvalid-logloss:0.32874\tvalid-roc_auc:0.89080\n",
      "[700]\tvalid-logloss:0.32855\tvalid-roc_auc:0.89091\n",
      "[800]\tvalid-logloss:0.32847\tvalid-roc_auc:0.89097\n",
      "[900]\tvalid-logloss:0.32842\tvalid-roc_auc:0.89100\n",
      "[1000]\tvalid-logloss:0.32839\tvalid-roc_auc:0.89103\n",
      "[1100]\tvalid-logloss:0.32834\tvalid-roc_auc:0.89105\n",
      "[1200]\tvalid-logloss:0.32834\tvalid-roc_auc:0.89106\n",
      "[1300]\tvalid-logloss:0.32828\tvalid-roc_auc:0.89108\n",
      "[1400]\tvalid-logloss:0.32829\tvalid-roc_auc:0.89109\n",
      "[1500]\tvalid-logloss:0.32828\tvalid-roc_auc:0.89110\n",
      "[1600]\tvalid-logloss:0.32825\tvalid-roc_auc:0.89111\n",
      "[1700]\tvalid-logloss:0.32827\tvalid-roc_auc:0.89112\n",
      "[1800]\tvalid-logloss:0.32824\tvalid-roc_auc:0.89113\n",
      "[1900]\tvalid-logloss:0.32825\tvalid-roc_auc:0.89113\n",
      "[1999]\tvalid-logloss:0.32820\tvalid-roc_auc:0.89115\n",
      "폴드 4 점수 : 0.8911460717094896\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncc05\\anaconda3\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.53588\tvalid-roc_auc:0.80825\n",
      "[100]\tvalid-logloss:0.35286\tvalid-roc_auc:0.88702\n",
      "[200]\tvalid-logloss:0.33437\tvalid-roc_auc:0.88790\n",
      "[300]\tvalid-logloss:0.33142\tvalid-roc_auc:0.88893\n",
      "[400]\tvalid-logloss:0.33013\tvalid-roc_auc:0.88970\n",
      "[500]\tvalid-logloss:0.32930\tvalid-roc_auc:0.89021\n",
      "[600]\tvalid-logloss:0.32897\tvalid-roc_auc:0.89046\n",
      "[700]\tvalid-logloss:0.32882\tvalid-roc_auc:0.89058\n",
      "[800]\tvalid-logloss:0.32874\tvalid-roc_auc:0.89063\n",
      "[900]\tvalid-logloss:0.32866\tvalid-roc_auc:0.89068\n",
      "[1000]\tvalid-logloss:0.32865\tvalid-roc_auc:0.89070\n",
      "[1100]\tvalid-logloss:0.32858\tvalid-roc_auc:0.89074\n",
      "[1200]\tvalid-logloss:0.32857\tvalid-roc_auc:0.89075\n",
      "[1300]\tvalid-logloss:0.32855\tvalid-roc_auc:0.89076\n",
      "[1400]\tvalid-logloss:0.32854\tvalid-roc_auc:0.89077\n",
      "[1500]\tvalid-logloss:0.32855\tvalid-roc_auc:0.89078\n",
      "[1600]\tvalid-logloss:0.32850\tvalid-roc_auc:0.89079\n",
      "[1700]\tvalid-logloss:0.32848\tvalid-roc_auc:0.89081\n",
      "[1800]\tvalid-logloss:0.32848\tvalid-roc_auc:0.89081\n",
      "[1900]\tvalid-logloss:0.32851\tvalid-roc_auc:0.89082\n",
      "[1999]\tvalid-logloss:0.32847\tvalid-roc_auc:0.89082\n",
      "폴드 5 점수 : 0.8908209211555329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds_xgb = np.zeros(X.shape[0]) \n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds_xgb = np.zeros(X_test.shape[0]) \n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    # XGBoost 전용 데이터셋 생성 \n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params=max_params, \n",
    "                          dtrain=dtrain,\n",
    "                          num_boost_round=2000,\n",
    "                          evals=[(dvalid, 'valid')],\n",
    "                          maximize=True,\n",
    "                          feval=roc_auc,\n",
    "                          early_stopping_rounds=200,\n",
    "                          verbose_eval=100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter = xgb_model.best_iteration\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds_xgb += xgb_model.predict(dtest,\n",
    "                                            iteration_range=(0, best_iter))/folds.n_splits\n",
    "    \n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n",
    "    oof_val_preds_xgb[valid_idx] = xgb_model.predict(dvalid, \n",
    "                                                      iteration_range=(0, best_iter))\n",
    "    \n",
    "    # 검증 데이터 예측확률에 대한 점수\n",
    "    score = roc_auc_score(y_valid, oof_val_preds_xgb[valid_idx])\n",
    "    print(f'폴드 {idx+1} 점수 : {score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf43e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test데이터에대한 roc넓이 0.8921102020425653\n"
     ]
    }
   ],
   "source": [
    "print('test데이터에대한 roc넓이', roc_auc_score(y, oof_val_preds_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6762da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Exited']=oof_test_preds_xgb\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aab3d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#앙상블\n",
    "oof_test_preds=oof_test_preds_xgb*0.5+oof_test_preds_lgb*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db08dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Exited']=oof_test_preds\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
